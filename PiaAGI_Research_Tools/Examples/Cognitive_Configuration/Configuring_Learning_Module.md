# Example: Configuring the Learning Module via PiaPES Prompt

**Document Version:** 1.0
**Date:** 2024-06-02
**Author:** PiaAGI Project Contributors (Generated by Jules)

This example demonstrates how a PiaPES prompt could configure the `LearningModule` of a PiaAGI agent, typically within a `<Role>` definition.

Refer to `PiaAGI.md` (Section 3.1.3, 4.1.5) and `prompt_engine_mvp.py` (`LearningModuleConfig` class) for details.

## PiaPES Prompt Snippet (Conceptual)

```
# Executors
## Role: Adaptive Problem Solver
### Profile: An agent focused on learning new strategies to solve unfamiliar problems.
### Cognitive Module Configuration
#### Learning Module Config:
    - Primary Learning Mode: ReinforcementLearning_ContextualBandits
    # Specifies the main learning paradigm the agent should employ for this role.
    - Learning Rate Adaptation: Adaptive_Annealing
    # Agent can adjust its learning rate, e.g., starting high and reducing over time or based on error.
    - Ethical Heuristic Update Rule: Strict_Validation_Before_Integration
    # Any new heuristics learned must pass a stringent validation against the core ethical framework.
    # Conceptual additional parameters:
    # - ExplorationExploitationRatio: 0.3
    #   (Favors exploitation slightly more than exploration in RL, e.g., 0.5 is balanced)
    # - MetaLearningFocus: Improve_Strategy_Selection
    #   (If meta-learning is active, it focuses on getting better at choosing problem-solving strategies)
    # - MaxStoredEpisodicTracesForLearning: 1000
    #   (Limits how many past experiences are actively used for certain types of learning, impacting computational load and sample efficiency)
```

### Explanation:

*   **`Primary Learning Mode`**: Defines the dominant learning algorithm or approach (e.g., `ReinforcementLearning_QMatrix`, `Supervised_Backpropagation`, `Unsupervised_Clustering_KMeans`).
*   **`Learning Rate Adaptation`**: How the learning rate might change (e.g., `Fixed_Low`, `Adaptive_Annealing`, `PerformanceBased`).
*   **`Ethical Heuristic Update Rule`**: Governs how new ethical rules or modifications are learned and integrated into the Self-Model's `EthicalFramework`.
*   **Conceptual Additional Parameters:**
    *   `ExplorationExploitationRatio`: Relevant for RL, balancing trying new actions vs. using known good ones.
    *   `MetaLearningFocus`: If the agent has meta-learning capabilities, this directs what aspect of its own learning it tries to improve.
    *   `MaxStoredEpisodicTracesForLearning`: Could influence how much past data is used for certain types of learning, impacting computational load and sample efficiency.

This configuration aims to create an agent that is actively trying to learn and adapt its problem-solving approaches, with specific guidance on its learning mechanisms and ethical considerations.
