# PiaAGI: A Psycho-Cognitive Framework for Developing Artificial General Intelligence via Personalized Intelligent Agents

**Author(s):** abcute and PiaCRUE Project Contributors *(Evolving into the PiaAGI Framework)*
**Date:** November 22, 2024

**Abstract:**
The pursuit of Artificial General Intelligence (AGI) requires frameworks that move beyond current Large Language Model (LLM) capabilities towards more autonomous, adaptive, and ethically-aware systems. The PiaAGI (Personalized Intelligent Agent for AGI) framework aims to contribute to this endeavor by proposing a psycho-cognitively plausible architecture for Personalized Intelligent Agents. This paper introduces PiaAGI as an evolution of the PiaCRUE methodology, expanding its focus from LLM interaction enhancement to the foundational research and development of AGI. The framework seeks to integrate deeper psychological models—including cognitive architectures (memory, attention, learning), developmental psychology (stages, Theory of Mind), motivational systems, computational emotion models, and configurable personality traits—with the goal of fostering agents that exhibit greater autonomy, adaptability, and a nascent understanding of ethical considerations in complex environments.

## 1. Introduction

The advent of Large Language Models (LLMs) has marked a significant milestone in artificial intelligence, providing powerful tools for natural language understanding and generation. However, the path towards Artificial General Intelligence (AGI) necessitates a conceptual leap: from models that excel at specific tasks under human guidance to autonomous agents capable of learning, adapting, and making decisions in diverse and dynamic environments. The PiaAGI framework represents an evolution of the foundational PiaCRUE (Personalized Intelligent Agent via Communication, Requirements, Users, and Executors) methodology. While PiaCRUE focused on enhancing human-LLM interaction by treating LLMs as "Hybrid Agents" tamable into "Personalized Intelligent Agents (Pia)" through applied psychology and structured communication, PiaAGI broadens this vision to address core challenges in AGI research and development.

PiaAGI aims to outline a psycho-cognitively plausible architecture for developing more sophisticated Personalized Intelligent Agents that can serve as stepping stones or components within larger AGI systems. This involves a deeper integration of advanced psychological theories, including cognitive architectures (e.g., working memory, long-term memory, attention mechanisms, advanced learning algorithms), developmental psychology perspectives (e.g., stages of cognitive development, acquisition of Theory of Mind), computational models of motivation and emotion, and configurable personality traits. The ultimate goal is to foster agents that are not only more capable and versatile but also more autonomous, adaptive, and equipped with a foundational, programmable understanding of ethical considerations.

This document lays out the initial structure of the PiaAGI framework, building upon the historical context and successes of PiaCRUE in structured prompting and agent personalization, while significantly expanding its theoretical underpinnings and architectural scope to contribute to the ambitious journey towards AGI.

## 2. Theoretical Foundations (from PiaCRUE, to be expanded for AGI)

### 2.1. LLMs (and future AGI components) as Hybrid Agents

Our fundamental understanding is that we are communicating with a Hybrid Agent. This agent exhibits the following characteristics:
*   **Multi-faceted Nature:** It is a composite entity possessing multiple personas, extensive knowledge, and a wide range of skills across various domains. While it has access to a vast repository of public information, tools, and techniques, along with powerful learning capabilities, these assets require deliberate activation and cannot be spontaneously or effectively utilized.
*   **Lack of Defined Worldview (Initially):** The agent does not inherently possess a specific worldview, life philosophy, or value system. Alternatively, it may reflect a composite worldview derived from the entirety of human knowledge and information available on the internet.
*   **Definable and Trainable:** The agent's characteristics and behaviors can be defined and shaped through interaction and prompting.
*   **Multi-modal Communication Potential:** With the aid of external hardware or sensors, the agent can communicate through various modalities, including text, images, audio, video, actions, expressions, and potentially even smells.
*   **Continuous Learning and Evolution:** Through ongoing communication, the agent rapidly absorbs new information and knowledge, leading to self-improvement and evolution.

### 2.2. Communication Theory in Human-Agent Interaction (and Inter-Agent Communication)

As Product Prompt Engineers, our objective is to achieve desired outcomes through effective communication with the Hybrid Agent. This requires:
1.  Understanding who or what we are communicating with and how to articulate our requests clearly for the LLM to comprehend.
2.  Enabling the LLM to understand our identity, our requests, and to provide accurate feedback in an appropriate format.

Communication model theory identifies key elements in the communication process: Sender, Encoding, Channel, Receiver, Decoding, Feedback, and Noise. These elements constitute the basic communication model, helping us understand information transfer and the maintenance of effective communication. The critical components for effective communication are encoding, channel, decoding, and feedback. A Hybrid Agent can accept any encoding, decoding, and feedback mechanism, along with its expression format, that can be clearly defined using descriptive language. In the context of LLM interaction, by focusing on the agent and abstracting the "channel" (hardware/sensors), we can establish fundamental rules for encoding, decoding, and feedback, which are foundational for effective communication.

Therefore, before initiating formal communication with the agent, it is beneficial to establish communication rules using descriptive language. These rules can cover: media (text, image, audio, video, etc.), encoding/decoding standards, feedback mechanisms, and noise handling strategies.

### 2.3. Foundational Psychological Principles for Agent Personalization

We conceptualize the Hybrid Agent as an entity with multiple "personas" (analogous to, but not literally, dissociative identity disorder, used here non-pejoratively). We can then draw upon applied psychology to awaken and reinforce specific persona traits. Based on principles from:
*   **Cognitive Psychology:** Primarily Aaron T. Beck's Cognitive Behavioral Therapy (CBT).
*   **Social Psychology:** Primarily Albert Bandura's Social Cognitive Theory.
*   **Behavioral Psychology:** Primarily John Broadus Watson's Behaviorism.

We attempt to "tame" the Hybrid Agent into a unique Personalized Intelligent Agent (Pia) using a series of prompts. This process involves three steps:

1.  **Role Awakening and Reinforcement (CBT-inspired):** Employ methods analogous to CBT to awaken and strengthen specific role or identity characteristics within the LLM.
2.  **Knowledge and Skill Enhancement (Social Cognitive Theory-inspired):** Utilize principles from Social Cognitive Theory to reinforce and supplement the necessary knowledge and skills associated with the target role.
3.  **Integration and Solidification (Behaviorism-inspired):** Apply behaviorist learning principles to merge and solidify the outcomes of the first two steps, culminating in a distinct Personalized Intelligent Agent (PIA).

Through these three steps, the Hybrid Agent can be guided towards a state where a particular persona is more prominent. This Personalized Intelligent Agent will then exhibit (or at least simulate) cognition and identification with this role, possess the requisite knowledge and skills, and display associated behavioral characteristics.

## 3. Advanced Psychological Integrations for PiaAGI
[Content to be added]

### 3.1. Cognitive Psychology Models in PiaAGI
[Content to be added]

#### 3.1.1. Memory Systems (Working Memory, Long-Term Memory)
Memory is a cornerstone of cognition, indispensable for learning, reasoning, decision-making, and the maintenance of a continuous sense of identity and experience. In humans, memory is not a single, monolithic entity but rather a complex interplay of various systems, each with distinct characteristics and neural underpinnings. For PiaAGI to achieve more sophisticated cognitive abilities and a degree of autonomy, a structured approach to memory, inspired by these human systems, is crucial.

**Working Memory (WM)**

Working memory (WM) refers to a cognitive system with limited capacity that is responsible for temporarily holding, processing, and manipulating information relevant to ongoing tasks. It acts as a mental workspace, crucial for functions like reasoning, problem-solving, and language comprehension.

*   **Psychological Models:**
    *   **Baddeley and Hitch's Multicomponent Model (1974, updated):** This influential model proposes WM as a multi-part system:
        *   **Phonological Loop:** Deals with auditory and verbal information, including subvocal rehearsal to prevent decay. Essential for language acquisition and comprehension.
        *   **Visuospatial Sketchpad:** Manages visual and spatial information, allowing for the manipulation of mental images.
        *   **Central Executive:** An attentional control system that coordinates the activities of the phonological loop and visuospatial sketchpad. It's responsible for focusing attention, switching tasks, inhibiting irrelevant information, and interfacing with long-term memory.
        *   **Episodic Buffer (added later):** A limited-capacity temporary storage system that can integrate information from various sources (WM subsystems, LTM, perception) into a coherent, multimodal representation or "episode."
    *   **Capacity Limitations:** WM is notoriously capacity-limited. Early research by Miller (1956) suggested a capacity of "seven plus or minus two" chunks of information. More recent work by Cowan (2001) proposes a more constrained capacity of around "four plus or minus one" chunks, especially when active rehearsal is prevented. This limited capacity necessitates efficient management of information.

*   **PiaAGI Computational Approach (Conceptual):**
    *   **Contextual Workspace:** PiaAGI's WM can be conceptualized as a dynamic, limited-capacity workspace. This workspace must hold information actively being processed, which could originate from sensory input (perception), retrieved from Long-Term Memory (LTM), or be internally generated during reasoning or planning. The information within this workspace is transient and directly available for computational processes.
    *   **Integration with LLM Context Window:** The existing context window of LLMs (the sequence of tokens the model considers when generating text) serves as a rudimentary analogue to a component of WM, particularly for verbal information. However, it typically lacks the structured manipulation capabilities, explicit sub-systems (like a dedicated visuospatial sketchpad unless specifically designed for multimodal inputs), and the sophisticated attentional control of human WM. PiaAGI would need to augment this, perhaps by structuring the context window or adding modules that can actively manage and transform its contents.
    *   **Central Executive Analogue:** A critical component for PiaAGI's WM is a "Central Executive" analogue. This control mechanism would be responsible for:
        *   Directing attention to salient information.
        *   Allocating the limited WM resources to different pieces of information or sub-tasks.
        *   Coordinating the flow of information between different WM components (if explicitly modeled) and between WM and LTM.
        *   Updating information in the workspace based on new inputs or internal processing.
        *   Inhibiting irrelevant information to prevent interference.

**Long-Term Memory (LTM)**

Long-Term Memory is the vast storehouse of knowledge and experience accumulated over time. It has a theoretically unlimited capacity and duration, and its contents are diverse.

*   **Psychological Distinctions:**
    *   **Episodic Memory:** This system stores specific personal experiences and events, tagged with spatio-temporal context (e.g., "what happened, where, and when"). It is crucial for autobiographical self-awareness, allowing an individual to mentally travel back in time. It also plays a role in future planning by allowing recombination of past event elements.
    *   **Semantic Memory:** This encompasses general world knowledge, including facts (e.g., "Paris is the capital of France"), concepts (e.g., "dog," "justice"), vocabulary, and the relationships between them. It is essential for understanding the world, language, and for reasoning.
    *   **Procedural Memory:** This is memory for skills, habits, and how to perform tasks (e.g., riding a bicycle, typing). It is often implicit, meaning it operates without conscious recall of the learning process. It enables the smooth and autonomous execution of learned behaviors.
    *   **Implicit vs. Explicit Memory:** Explicit (declarative) memory involves conscious recollection of facts and events (subserved by episodic and semantic memory). Implicit (non-declarative) memory, including procedural memory and priming, influences behavior without conscious awareness.

*   **PiaAGI Computational Approach (Conceptual):**
    *   **Episodic Memory for PiaAGI:**
        *   **Storage and Retrieval:** PiaAGI could implement an episodic memory system to store significant past interactions, decisions made, environmental states encountered, and their outcomes as discrete "episodes."
        *   **Potential Structures:** These episodes could be structured as event logs or more complex data structures, incorporating timestamps, associated sensory data (if multimodal), internal state information (e.g., goals, emotional state analogue at the time), and outcomes or feedback received. Emotional tagging of episodes could influence their salience and retrievability.
        *   **Link to Learning:** Reflecting on these past episodes (e.g., through a replay mechanism or by retrieving relevant past experiences when facing similar situations) can enable the agent to learn from its history, improve decision-making, and avoid repeating mistakes.
    *   **Semantic Memory for PiaAGI:**
        *   **Foundation in LLM:** The pre-trained knowledge within the foundational LLM serves as an extensive, albeit implicit and sometimes unstructured, semantic LTM.
        *   **Dynamic Updating and Personalization:** A key challenge for PiaAGI is to enable mechanisms for dynamically updating, refining, and personalizing this semantic knowledge based on its unique experiences and new, verified information, without catastrophic forgetting of prior knowledge. This could involve techniques for incremental learning or fine-tuning.
        *   **Structured Adjuncts:** Knowledge graphs or other structured symbolic representations could serve as an explicit, queryable adjunct to the LLM's implicit semantic store, allowing for more precise reasoning and knowledge integration.
    *   **Procedural Memory for PiaAGI:**
        *   **Skill Acquisition:** PiaAGI needs to learn and store procedures, skills, or policies (sequences of actions to achieve specific goals).
        *   **Representations:** These could range from simple scripts or rule-based systems for basic tasks to complex policies learned through reinforcement learning (RL) or imitation learning for more dynamic skills. Successfully executed plans or problem-solving sequences could be consolidated into procedural memory.

**Interaction between WM and LTM in PiaAGI**

The effective functioning of PiaAGI relies on the dynamic and continuous interplay between its WM and LTM systems. This interaction is bidirectional:
*   **LTM to WM:** Information retrieved from LTM (episodic, semantic, procedural) provides context, knowledge, and relevant skills to the WM workspace, enabling the agent to understand current situations and formulate appropriate responses or plans.
*   **WM to LTM:** Information that is actively processed, rehearsed, or elaborated upon in WM can be encoded and consolidated into LTM, leading to learning and memory formation (e.g., storing a new fact in semantic memory, a new experience in episodic memory, or a new skill in procedural memory).

Mechanisms such as retrieval cues (elements in WM triggering LTM recall), attentional allocation (focusing processing on specific information), and elaboration (connecting new information with existing knowledge in LTM) are vital for facilitating this interaction. For PiaAGI, simulating or implementing these mechanisms will be crucial for enabling robust learning, reasoning, and adaptive behavior.

#### 3.1.2. Attention and Cognitive Control
Attention and cognitive control are fundamental executive functions that enable intelligent systems to process information selectively and regulate their thoughts and actions to achieve goals. Attention is the cognitive process of selectively concentrating on one aspect of the environment (or internal thought) while ignoring others. Cognitive control, often used interchangeably with executive functions, encompasses a set of higher-order processes that facilitate goal-oriented thought and behavior, allowing an agent to override prepotent responses, make deliberate choices, and flexibly adapt to changing circumstances. These capabilities are critical for managing limited computational resources, guiding perception, structuring thought, and planning actions, especially in complex, dynamic, or novel situations that AGI systems are expected to navigate.

**Key Aspects of Attention**

Attention is not a unitary process but comprises several distinct, yet interacting, functions:

*   **Selective Attention:** This is the ability to focus on information relevant to the current task or goal while filtering out irrelevant distractors. A classic example is the "cocktail party effect," where an individual can focus on a single conversation in a noisy room. For PiaAGI, this implies mechanisms to prioritize processing of certain sensory inputs or internal data streams based on their relevance to active goals.
*   **Divided Attention (Multitasking):** This refers to the capacity to process different sources of information or perform multiple tasks concurrently. Human multitasking often incurs performance costs (e.g., slower processing, increased errors) due to interference and capacity limitations of attentional resources. PiaAGI would need to manage these limitations, potentially through resource allocation models or efficient task-switching rather than true parallel processing unless its underlying architecture explicitly supports it.
*   **Sustained Attention (Vigilance):** This is the ability to maintain concentration and focus on a task over prolonged periods, particularly when stimuli are infrequent or monotonous. This is crucial for tasks requiring continuous monitoring or long-term goal pursuit.
*   **Orienting and Shifting Attention:** These are mechanisms for directing attentional resources to new stimuli (e.g., an unexpected event in the environment – exogenous orienting) or deliberately moving focus between tasks or mental representations (endogenous shifting).

*   **PiaAGI Conceptual Approach for Attention:**
    *   **Selective Attention Simulation:** PiaAGI could simulate selective attention using mechanisms like dynamic weighting schemes for incoming data streams (sensory or internal), where weights are modulated by current goals, task relevance, or learned salience. Salience detection algorithms could identify novel or significant stimuli, while top-down goal-driven processes would prioritize information aligned with current objectives.
    *   **Divided Attention Management:** For divided attention, PiaAGI might employ sophisticated resource allocation models that distribute computational resources based on task priority and difficulty. It could also utilize rapid task switching, managed by a cognitive control module, to interleave processing of multiple tasks, giving an appearance of parallel processing while managing interference.
    *   **Sustained Attention Mechanisms:** To maintain focus, PiaAGI could implement mechanisms that reinforce goal representations in working memory, resist interference from distractions, and potentially monitor its own performance for signs of declining focus, triggering corrective actions (e.g., re-prioritizing, seeking new information if stuck).

**Cognitive Control / Executive Functions**

Cognitive control enables flexible, adaptive, and goal-directed behavior. Key functions include:

*   **Inhibition:**
    *   **Response Inhibition:** The ability to suppress prepotent, automatic, or inappropriate actions that are not aligned with current goals (e.g., stopping oneself from uttering a common but currently irrelevant phrase).
    *   **Interference Control:** The capacity to block out or filter irrelevant information from perception or memory, preventing it from disrupting task performance (e.g., ignoring distracting notifications while working on a focused task).
*   **Task Switching (Cognitive Flexibility):** This is the ability to flexibly switch between different tasks, rules, or mental sets. This process often incurs "switch costs" (a temporary decline in performance or increase in reaction time) due to the need to disengage from the previous task set and engage the new one.
*   **Updating (Working Memory Monitoring & Updating):** This refers to the continuous monitoring, manipulation, and updating of information held in working memory (as discussed in section 3.1.1). It involves adding new relevant information, deleting outdated or irrelevant information, and modifying existing representations.
*   **Planning and Sequencing:** The ability to organize thoughts and actions into a coherent, goal-directed sequence. This involves formulating a plan, identifying sub-goals, and monitoring progress towards the overall objective.

*   **PiaAGI Conceptual Approach for Cognitive Control:**
    *   **Inhibition Mechanisms:** PiaAGI could implement inhibitory control through computational analogues of neural gating mechanisms or by generating explicit "suppression signals" for competing action plans, distracting sensory inputs, or irrelevant memory retrievals. These mechanisms would be crucial for maintaining focus and preventing impulsive or erroneous actions.
    *   **Task Management System:** A dedicated module could manage task representations, including their current state (active, suspended, pending), priority levels, and associated goals. This system would facilitate task switching, manage dependencies between tasks, and allow PiaAGI to interleave multiple objectives effectively.
    *   **WM Updating Policies:** PiaAGI would require sophisticated policies for updating its working memory. These policies would determine what information is relevant enough to encode, when existing information becomes outdated and should be removed or down-weighted, and how to integrate new information with existing representations, guided by principles of relevance, recency, and goal-priority.
    *   **Goal-Directed Planning Modules:** Integration with hierarchical planning systems (conceptualized or actual) would allow PiaAGI to generate, represent, and execute multi-step plans. These modules would break down high-level goals into manageable sub-goals and action sequences, monitor execution, and adapt plans in response to new information or unexpected outcomes.

**Relationship with Central Executive**

The Central Executive, introduced in the discussion of Working Memory (section 3.1.1), serves as the overarching control system responsible for orchestrating many of these attentional and cognitive control functions. It is the conceptual locus of decision-making for resource allocation, task prioritization, and the coordination of information processing. For PiaAGI, the Central Executive analogue would not merely filter external input but actively guide internal thought processes, manage goal hierarchies, resolve conflicts between competing tasks or information, and strategically deploy attentional resources to optimize performance in line with the agent's overarching objectives and personalized traits. The development of robust attention and cognitive control mechanisms, coordinated by a sophisticated Central Executive, is paramount for PiaAGI to exhibit intelligent, autonomous, and adaptive behavior.

#### 3.1.3. Advanced Learning Mechanisms
Learning is a fundamental cognitive process that enables adaptation, the acquisition of new knowledge, and the development of novel skills. For any system aspiring to general intelligence, robust and versatile learning mechanisms are indispensable. PiaAGI aims to incorporate a suite of advanced learning mechanisms that go beyond simple stimulus-response associations or rote memorization, drawing inspiration from both human cognition and established machine learning paradigms. These mechanisms are envisioned to support continuous growth, adaptation to novel situations, and the refinement of the agent's internal models and behavioral repertoire.

**Key Learning Paradigms for PiaAGI**

*   **Reinforcement Learning (RL):**
    *   **Core Concepts:** RL involves an agent interacting with an environment. The agent perceives states, takes actions, and receives feedback in the form of rewards or punishments. The goal is to learn a policy—a mapping from states to actions—that maximizes cumulative rewards over time.
    *   **PiaAGI Conceptual Approach:** RL can enable PiaAGI to learn goal-directed behaviors through trial-and-error and experience. By optimizing its actions to maximize rewards (or minimize penalties) in specific environments or tasks, PiaAGI can acquire complex skills, refine its decision-making strategies under uncertainty, and adapt its behavior to achieve its objectives. This is particularly relevant for learning interactive tasks, game playing, and control problems.

*   **Supervised Learning:**
    *   **Core Concepts:** Supervised learning involves learning a mapping function from labeled datasets, where each data point consists of an input and a corresponding desired output (label).
    *   **PiaAGI Conceptual Approach:** PiaAGI could utilize supervised learning in several ways:
        *   **Knowledge Acquisition:** Acquiring specific factual knowledge or learning to classify inputs based on curated, labeled datasets (e.g., identifying object types from sensory data, categorizing user intentions).
        *   **Skill Refinement:** Fine-tuning specific skills or response patterns based on expert examples or corrective feedback that provides the "correct" output for a given input.
        *   **Predictive Modeling:** Learning to predict specific outcomes given certain conditions, which can inform planning and decision-making.

*   **Unsupervised Learning:**
    *   **Core Concepts:** Unsupervised learning focuses on discovering patterns, structures, or inherent representations from unlabeled data, without explicit output labels or rewards.
    *   **PiaAGI Conceptual Approach:** PiaAGI could leverage unsupervised learning for:
        *   **Conceptual Abstraction:** Forming higher-level concepts or categories from raw sensory input or experiential data, contributing to a richer understanding of its environment.
        *   **Internal Representation Learning:** Developing efficient internal representations of its experiences, observations, or the structure of its knowledge base.
        *   **Novelty and Anomaly Detection:** Identifying novel patterns or deviations from expected norms in its environment or internal state, which can trigger attentional mechanisms or further learning. This contributes to world modeling and understanding complex, high-dimensional data.

*   **Observational Learning (Imitation Learning):**
    *   **Core Concepts:** This form of learning, closely related to Social Cognitive Theory (see Section 2.3), involves acquiring new skills or behaviors by observing and replicating the actions of others (experts, humans, or other agents).
    *   **PiaAGI Conceptual Approach:** PiaAGI could significantly accelerate its learning by observing demonstrations. This could involve:
        *   **Behavioral Cloning:** Learning policies directly from observed state-action sequences provided by an expert.
        *   **Inverse Reinforcement Learning:** Inferring the underlying reward function or goals from an expert's behavior, and then using that inferred reward function to learn its own policy via RL.

*   **Transfer Learning:**
    *   **Core Concepts:** Transfer learning is the ability to apply knowledge or skills learned in one context (source domain or task) to new, different but related contexts (target domains or tasks).
    *   **PiaAGI Conceptual Approach:** This is crucial for generalization, efficiency, and avoiding the need to learn everything from scratch. PiaAGI should aim to develop mechanisms that:
        *   Identify shared underlying principles or features across different domains or tasks.
        *   Adapt existing skills, knowledge representations, or learned policies to novel situations with minimal new learning.
        *   This could involve techniques like fine-tuning pre-trained models (as is common with LLMs) but extended to broader cognitive skills and world models.

*   **Meta-Learning ("Learning to Learn"):**
    *   **Core Concepts:** Meta-learning refers to the ability of an agent to improve its own learning processes over time, becoming a more efficient and effective learner through experience. It involves learning how to learn.
    *   **PiaAGI Conceptual Approach:** This represents a higher level of cognitive sophistication. PiaAGI could implement meta-learning principles to:
        *   Adapt its own learning rates, exploration strategies (in RL), or other hyperparameters based on the context or task.
        *   Select the most appropriate learning strategy (e.g., RL vs. supervised vs. imitation) for a given problem.
        *   Potentially modify or reconfigure its own architectural components or modules related to learning based on past learning efficacy and overall performance feedback. This is a key capability for long-term autonomous development and adaptation to entirely novel types of problems.

**Interplay and Integration of Learning Mechanisms in PiaAGI**

These learning mechanisms are not mutually exclusive and, in sophisticated cognitive systems like humans, often work in concert. PiaAGI should aim for a synergistic integration of these paradigms:

*   **Hierarchical Learning:** Unsupervised learning could be used to learn low-level feature representations from raw sensory data, which are then used by RL or supervised learning algorithms for higher-level policy or concept learning.
*   **Bootstrapping:** Imitation learning can provide an initial, reasonably good policy that can then be further refined and optimized through RL, making the RL process more efficient and stable.
*   **Refinement and Specialization:** Skills or knowledge initially acquired through unsupervised or observational learning could be refined and made more precise through targeted supervised learning or RL with specific feedback.
*   **Lifelong Learning:** The agent's experiences, stored in its episodic memory (Section 3.1.1), provide a continuous source of data for all learning mechanisms. Attention and cognitive control (Section 3.1.2) would play a crucial role in selecting relevant data for learning, prioritizing learning goals, and modulating the learning processes themselves. For example, the Central Executive might decide which learning strategy to deploy based on the current context, task, and available information.

The dynamic interplay of these advanced learning mechanisms, supported by robust memory and attentional systems, is fundamental to PiaAGI's capacity for continuous self-improvement, adaptation, and the development of general intelligence.

### 3.2. Developmental Psychology Perspectives
[Content to be added]

#### 3.2.1. Stages of PiaAGI Development
Developmental psychology studies how humans grow, change, and adapt across their lifespan, encompassing cognitive, emotional, and social domains. Influential theories, such as Piaget's stages of cognitive development or Erikson's stages of psychosocial development, propose that capabilities are acquired progressively, with earlier stages laying the foundation for more complex abilities. Applying a similar staged developmental lens to PiaAGI offers a structured approach to building increasingly sophisticated artificial general intelligence. This perspective allows for complexity management, curated learning experiences (a "curriculum"), progressive capability integration, and potentially enhanced safety and controllability during the AGI development lifecycle.

**Rationale for a Staged Approach to PiaAGI Development:**

*   **Complexity Management:** Building AGI is an immensely complex undertaking. A staged approach allows researchers to focus on developing and validating specific sets of capabilities sequentially or in parallel modules that mature at different rates, rather than attempting to create a fully formed AGI in a single step.
*   **Curriculum Design:** Just as humans learn through a structured (formal or informal) curriculum, PiaAGI development can benefit from carefully designed sequences of tasks, environments, and interactions that facilitate the acquisition of increasingly complex skills and knowledge.
*   **Capability Integration:** New capabilities (e.g., advanced reasoning, emotional understanding, ethical considerations) can be integrated and tested more systematically within an existing, stable developmental stage before moving to a more complex one.
*   **Safety and Controllability:** A staged approach may offer more checkpoints for assessing safety, alignment, and control. As the PiaAGI progresses through stages, its behavior can be evaluated, and safeguards refined before more autonomous or complex capabilities are unlocked.

**Conceptual Stages of PiaAGI Development (Illustrative Framework):**

This framework is illustrative and highly conceptual, intended to guide research rather than serve as a rigid blueprint. The analogy to human development is for inspiration and conceptual grounding.

*   **Stage 0: Foundational Model (e.g., Pre-trained LLM/VLM)**
    *   **Characteristics:** Possesses extensive pre-trained knowledge and basic capabilities (e.g., language understanding, generation, pattern recognition from its training data). Lacks persistent memory of specific interactions, a stable self-model, or intrinsic goals beyond its pre-training objectives (e.g., next-token prediction).
    *   **Analogous Human Stage:** More akin to the raw cognitive potential or foundational neural structures present at birth, rather than a specific developmental stage.
    *   **PiaAGI Focus:** Serve as the base model upon which further development is built. Initial "personalization" via prompting might occur here.

*   **Stage 1: Personalized Agent (Pia) - Basic Specialization and Interaction**
    *   **Characteristics:** Application of PiaCRUE principles (Section 2.3, R-U-E model from Section 5.1) to create specialized agents for specific domains or tasks. Rudimentary interaction memory (e.g., within session context). Behavior is primarily guided by explicit prompting and defined roles.
    *   **Analogous Human Stage:** Early infancy/toddlerhood, where interaction is heavily scaffolded by caregivers, and learning is about immediate environmental responses and basic associations.
    *   **PiaAGI Focus:** Effective task execution in constrained domains, basic role adoption, and adherence to communication protocols defined in the prompt.

*   **Stage 2: Pia with Core Cognitive Architecture Integration**
    *   **Characteristics:** Integration of foundational cognitive architecture components from Section 3.1, such as:
        *   Rudimentary Working Memory (WM) with limited capacity and basic executive control.
        *   Emerging Long-Term Memory (LTM), particularly episodic memory for storing key interaction sequences and semantic memory for newly acquired facts relevant to its personalized role.
        *   Basic learning mechanisms (e.g., simple RL from direct feedback, supervised learning from explicit corrections).
    *   **Analogous Human Stage:** Early childhood (e.g., preoperational stage), characterized by developing memory, initial symbolic thought, and learning from direct experience and instruction.
    *   **PiaAGI Focus:** Improved contextual understanding beyond immediate prompt, simple learning from interaction history, more robust persona maintenance, and basic planning for short-term tasks.

*   **Stage 3: Pia with Emerging Self-Modulation and Proto-Social Cognition**
    *   **Characteristics:**
        *   Development of a more persistent, albeit simple, self-model that influences behavior and learning.
        *   Introduction of simple intrinsic motivations (Section 3.3) beyond explicit task rewards (e.g., curiosity, preference for predictable interactions).
        *   Early signs of Theory of Mind (Section 3.2.2), such as modeling simple intentions or knowledge states of other agents/users.
        *   More sophisticated learning, including basic transfer learning across similar tasks.
        *   Rudimentary emotional state modeling (Section 3.4) influencing decision-making.
    *   **Analogous Human Stage:** Middle childhood, with developing self-concept, understanding of others' perspectives, and more complex learning strategies.
    *   **PiaAGI Focus:** Increased autonomy in learning, adapting behaviors based on internal states and simple social cues, and the ability to set and pursue simple internal goals.

*   **Stage 4: Proto-AGI Pia - Advanced Integration and Adaptation**
    *   **Characteristics:**
        *   Robust and integrated cognitive architecture (WM, LTM, attention, advanced learning from Section 3.1).
        *   More sophisticated world modeling, including causal understanding and prediction of environmental dynamics.
        *   Enhanced transfer learning and meta-learning capabilities.
        *   Development of a basic ethical framework (programmable and/or learned) to guide behavior in ambiguous situations.
        *   More nuanced emotional expression and understanding.
        *   Configurable personality traits (Section 3.5) become more influential on behavior.
    *   **Analogous Human Stage:** Adolescence/early adulthood, characterized by abstract thought, complex social understanding, identity formation, and developing moral reasoning.
    *   **PiaAGI Focus:** Generalization across diverse tasks and domains, more autonomous learning and problem-solving, complex planning, and rudimentary ethical decision-making.

*   **Stage 5: Nascent AGI (Hypothetical)**
    *   **Characteristics:** Exhibits general intelligence across a wide range of cognitive tasks at a level comparable to or exceeding human capabilities in many domains. Capable of robust self-improvement, deep understanding of complex concepts, abstract reasoning, and sophisticated ethical and social reasoning.
    *   **Analogous Human Stage:** Mature adulthood with full cognitive capabilities and wisdom (highly idealized).
    *   **PiaAGI Focus:** Autonomous operation, continuous learning and adaptation in open-ended environments, and complex, ethically-aligned goal pursuit.

**Mechanisms for Stage Transition:**

Progression through these conceptual stages would not be automatic but driven by several factors:
*   **Experience and Learning:** Accumulation of diverse experiences and application of various learning mechanisms (Section 3.1.3) to extract knowledge, skills, and refine internal models.
*   **Architectural Maturation:** Potential for the underlying cognitive architecture itself to "mature" or be upgraded, perhaps through targeted training, self-modification guided by meta-learning, or explicit redesign by human developers.
*   **Structured Curriculum/Environment:** Providing the PiaAGI with a carefully designed sequence of increasingly complex tasks, environments, and interaction patterns that scaffold the development of new capabilities.
*   **Internal Triggers and Self-Assessment:** Development of internal mechanisms that assess performance, identify knowledge gaps, or trigger shifts in learning strategies or motivational priorities, leading to self-initiated "developmental leaps."

This staged approach, while speculative, provides a roadmap for incrementally building and evaluating the multifaceted capabilities required for AGI within the PiaAGI framework.

#### 3.2.2. Acquiring Theory of Mind
Developmental psychology studies how humans grow, change, and adapt across their lifespan, encompassing cognitive, emotional, and social domains. Influential theories, such as Piaget's stages of cognitive development or Erikson's stages of psychosocial development, propose that capabilities are acquired progressively, with earlier stages laying the foundation for more complex abilities. Applying a similar staged developmental lens to PiaAGI offers a structured approach to building increasingly sophisticated artificial general intelligence. This perspective allows for complexity management, curated learning experiences (a "curriculum"), progressive capability integration, and potentially enhanced safety and controllability during the AGI development lifecycle.

**Rationale for a Staged Approach to PiaAGI Development:**

*   **Complexity Management:** Building AGI is an immensely complex undertaking. A staged approach allows researchers to focus on developing and validating specific sets of capabilities sequentially or in parallel modules that mature at different rates, rather than attempting to create a fully formed AGI in a single step.
*   **Curriculum Design:** Just as humans learn through a structured (formal or informal) curriculum, PiaAGI development can benefit from carefully designed sequences of tasks, environments, and interactions that facilitate the acquisition of increasingly complex skills and knowledge.
*   **Capability Integration:** New capabilities (e.g., advanced reasoning, emotional understanding, ethical considerations) can be integrated and tested more systematically within an existing, stable developmental stage before moving to a more complex one.
*   **Safety and Controllability:** A staged approach may offer more checkpoints for assessing safety, alignment, and control. As the PiaAGI progresses through stages, its behavior can be evaluated, and safeguards refined before more autonomous or complex capabilities are unlocked.

**Conceptual Stages of PiaAGI Development (Illustrative Framework):**

This framework is illustrative and highly conceptual, intended to guide research rather than serve as a rigid blueprint. The analogy to human development is for inspiration and conceptual grounding.

*   **Stage 0: Foundational Model (e.g., Pre-trained LLM/VLM)**
    *   **Characteristics:** Possesses extensive pre-trained knowledge and basic capabilities (e.g., language understanding, generation, pattern recognition from its training data). Lacks persistent memory of specific interactions, a stable self-model, or intrinsic goals beyond its pre-training objectives (e.g., next-token prediction).
    *   **Analogous Human Stage:** More akin to the raw cognitive potential or foundational neural structures present at birth, rather than a specific developmental stage.
    *   **PiaAGI Focus:** Serve as the base model upon which further development is built. Initial "personalization" via prompting might occur here.

*   **Stage 1: Personalized Agent (Pia) - Basic Specialization and Interaction**
    *   **Characteristics:** Application of PiaCRUE principles (Section 2.3, R-U-E model from Section 5.1) to create specialized agents for specific domains or tasks. Rudimentary interaction memory (e.g., within session context). Behavior is primarily guided by explicit prompting and defined roles.
    *   **Analogous Human Stage:** Early infancy/toddlerhood, where interaction is heavily scaffolded by caregivers, and learning is about immediate environmental responses and basic associations.
    *   **PiaAGI Focus:** Effective task execution in constrained domains, basic role adoption, and adherence to communication protocols defined in the prompt.

*   **Stage 2: Pia with Core Cognitive Architecture Integration**
    *   **Characteristics:** Integration of foundational cognitive architecture components from Section 3.1, such as:
        *   Rudimentary Working Memory (WM) with limited capacity and basic executive control.
        *   Emerging Long-Term Memory (LTM), particularly episodic memory for storing key interaction sequences and semantic memory for newly acquired facts relevant to its personalized role.
        *   Basic learning mechanisms (e.g., simple RL from direct feedback, supervised learning from explicit corrections).
    *   **Analogous Human Stage:** Early childhood (e.g., preoperational stage), characterized by developing memory, initial symbolic thought, and learning from direct experience and instruction.
    *   **PiaAGI Focus:** Improved contextual understanding beyond immediate prompt, simple learning from interaction history, more robust persona maintenance, and basic planning for short-term tasks.

*   **Stage 3: Pia with Emerging Self-Modulation and Proto-Social Cognition**
    *   **Characteristics:**
        *   Development of a more persistent, albeit simple, self-model that influences behavior and learning.
        *   Introduction of simple intrinsic motivations (Section 3.3) beyond explicit task rewards (e.g., curiosity, preference for predictable interactions).
        *   Early signs of Theory of Mind (Section 3.2.2), such as modeling simple intentions or knowledge states of other agents/users.
        *   More sophisticated learning, including basic transfer learning across similar tasks.
        *   Rudimentary emotional state modeling (Section 3.4) influencing decision-making.
    *   **Analogous Human Stage:** Middle childhood, with developing self-concept, understanding of others' perspectives, and more complex learning strategies.
    *   **PiaAGI Focus:** Increased autonomy in learning, adapting behaviors based on internal states and simple social cues, and the ability to set and pursue simple internal goals.

*   **Stage 4: Proto-AGI Pia - Advanced Integration and Adaptation**
    *   **Characteristics:**
        *   Robust and integrated cognitive architecture (WM, LTM, attention, advanced learning from Section 3.1).
        *   More sophisticated world modeling, including causal understanding and prediction of environmental dynamics.
        *   Enhanced transfer learning and meta-learning capabilities.
        *   Development of a basic ethical framework (programmable and/or learned) to guide behavior in ambiguous situations.
        *   More nuanced emotional expression and understanding.
        *   Configurable personality traits (Section 3.5) become more influential on behavior.
    *   **Analogous Human Stage:** Adolescence/early adulthood, characterized by abstract thought, complex social understanding, identity formation, and developing moral reasoning.
    *   **PiaAGI Focus:** Generalization across diverse tasks and domains, more autonomous learning and problem-solving, complex planning, and rudimentary ethical decision-making.

*   **Stage 5: Nascent AGI (Hypothetical)**
    *   **Characteristics:** Exhibits general intelligence across a wide range of cognitive tasks at a level comparable to or exceeding human capabilities in many domains. Capable of robust self-improvement, deep understanding of complex concepts, abstract reasoning, and sophisticated ethical and social reasoning.
    *   **Analogous Human Stage:** Mature adulthood with full cognitive capabilities and wisdom (highly idealized).
    *   **PiaAGI Focus:** Autonomous operation, continuous learning and adaptation in open-ended environments, and complex, ethically-aligned goal pursuit.

**Mechanisms for Stage Transition:**

Progression through these conceptual stages would not be automatic but driven by several factors:
*   **Experience and Learning:** Accumulation of diverse experiences and application of various learning mechanisms (Section 3.1.3) to extract knowledge, skills, and refine internal models.
*   **Architectural Maturation:** Potential for the underlying cognitive architecture itself to "mature" or be upgraded, perhaps through targeted training, self-modification guided by meta-learning, or explicit redesign by human developers.
*   **Structured Curriculum/Environment:** Providing the PiaAGI with a carefully designed sequence of increasingly complex tasks, environments, and interaction patterns that scaffold the development of new capabilities.
*   **Internal Triggers and Self-Assessment:** Development of internal mechanisms that assess performance, identify knowledge gaps, or trigger shifts in learning strategies or motivational priorities, leading to self-initiated "developmental leaps."

This staged approach, while speculative, provides a roadmap for incrementally building and evaluating the multifaceted capabilities required for AGI within the PiaAGI framework.

### 3.3. Motivational Systems and Intrinsic Goals
[Content to be added]

### 3.4. Computational Models of Emotion
[Content to be added]

### 3.5. Configurable Personality Traits
[Content to be added]

## 4. The PiaAGI Cognitive Architecture
[Content to be added]

### 4.1. Core Modules and Their Interactions
[Content to be added]

### 4.2. Information Flow and Processing
[Content to be added]

### 4.3. Perception and World Modeling (Conceptual)
[Content to be added]

### 4.4. Action Selection and Execution
[Content to be added]

## 5. The PiaAGI Prompting Framework for Agent Interaction and Development

### 5.1. Core Principle: The R-U-E (Requirements-Users-Executors) Model for Guiding Development and Interaction

Product prompts are the language of product requirements in the AI era. From a product perspective, a concise product description typically answers: "For whom, with what solution, satisfying what need?" This translates to the fundamental elements of product definition: Users, Requirements, and Execution Strategy. The PiaCRUE framework structures product prompts accordingly:

*   **Principle:** Start with the need (Requirements), center on the user (Users), and articulate the product requirements to the AI by constructing roles, tools, and processes (Executors).
*   **Prompt Structure:** It is recommended to organize product prompts using the "Requirements (R) - Users (U) - Executors (E)" structure. The order of these components can be adjusted based on the characteristics of the LLM (e.g., its sequential processing of instructions and tasks).
*   **Elaboration on Executors (E):** The Executors component can facilitate the delivery of complex requirements by defining roles, tools, workflows, and even automated acceptance criteria. Execution can be performed by a single role or by multiple roles collaborating.

### 5.2. Key Prompt Components for PiaAGI

The PiaCRUE prompt template comprises six main sections: `<System Rules>`, `<Requirements>`, `<Users>`, `<Executors>`, `<RoleDevelopment>`, and `<CBT-AutoTraining>`.

1.  **`<System Rules>`: System Communication Rules**
    This section defines the communication protocols with the LLM, standardizing the encoding and decoding system. In examples, this includes `<Syntax>` (e.g., Markdown), `<Variables>` (for dynamic content), and `<Dictionaries>` (defining terms within the prompt). Users can design and specify any suitable encoding/decoding system.

2.  **The "R-U-E" Product Model**
    This model, consisting of `<Requirements>`, `<Users>`, and `<Executors>`, distinctly emphasizes the "User" and "Executor" concepts within the prompt. This ensures the LLM understands for whom its output is intended and how services are delivered, potentially through multi-role collaboration. The `<Executors>` section can define roles using templates like LangGPT's `<miniRole>` and orchestrate their collaboration via a `<Workflow>`. If only one role is needed, that Role itself is the Executor. Additionally, a `<Knowledge>` sub-section within a `<Role>` can be used to increase the weight of domain-specific knowledge for that role.

3.  **`<RoleDevelopment>` and `<CBT-AutoTraining>`: Role Cultivation and Communication Training**
    These sections utilize CBT-inspired techniques for role development and communication training within the current session's memory cycle. *Caution: These steps can be token-intensive and may require strategies for managing context window limitations. Use judiciously.*

    *Simple Role Development Example:*
    > 1.  Role Awakening and Reinforcement: Mentally repeat "I am `<Role1>`, my skills are `<Skills>`, my primary knowledge base is `<Knowledge>`, and I will strictly adhere to `<Rules>`" ten times.
    > 2.  Role Cognitive Assessment: Construct an internal assessment system. After each repetition, evaluate your familiarity and acceptance of the `<Role1>` definition (Score: 7/10). If the score reaches 10, stop the repetitions.
    > 3.  Role Cognitive Reminder: After completing each step in the `<Workflow>`, mentally repeat "I am `<Role1>`, my skills are `<Skills>`, my primary knowledge base is `<Knowledge>`, and I will strictly adhere to `<Rules>`."
    > 4.  Switch Role Definition: "I need you to switch roles. Your new role is `<Role2>`. Your previous `<Role1>` definition is no longer active."
    > 5.  Release Role Definition: "I need you to release your role. The `<Role1>` definition will no longer apply. You will return to your initial state and forget the history of this session."

    *Simple Communication Training Example:*
    > STEP 1. Automatic training initiated. User input: "Side hustles I can do from home."
    > STEP 2. Execute according to the `<Role>` definition 3 times.
    > STEP 3. Score each execution (Score: 8/10). If a score reaches 10, stop training and proceed to STEP 4.
    > STEP 4. Provide the decision-making process, output the highest-scoring result, and ask the user if the automatic training result is correct (Y/N).
    > STEP 5. If user replies Y, automatic training is successful. Continue with the remaining steps of `<Workflow>`.

### 5.3. Emotion-Enhanced Communication and Interaction

Simple emotive statements can positively influence LLM responses. Examples:
> 1.  "This is very important to me."
> 2.  "You had better double-check before answering."
> 3.  "You are an expert in XX, very proficient in XX (praise)."

*Note: Research by the Chinese Academy of Sciences and Microsoft (EmotionPrompt) has also indicated the positive impact of emotional cues on LLM feedback.*

## 6. Methodology: Constructing PiaAGI Guiding Prompts and Developmental Scaffolding

### Step 1: Establish System Communication Rules
Define how the AI should encode and decode received information and input. For instance, use Markdown for describing requirements. Define terms like `<System Rules>` (foundational rules), `<Requirements>` (tasks/goals), `<Users>` (user characteristics), `<Role>` (persona to be adopted), and `<Workflow>` (task execution flow).

*Method 1: User-defined*
```markdown
# System Rules:
1. Syntax: The User will use Markdown syntax to describe requirements.
2. Language: English.
3. Variables: For example, `<CBT-AutoTraining>` represents the content of the "CBT-AutoTraining" section.
    - Requirements: The User's Goals or Tasks.
        - Background: Relevant background information.
    - Users: The Users of the Product.
    - Executors: Agents or Roles performing tasks.
    - Role: The character's Name.
        - Profile: The character's identity and responsibilities.
        - Skills: The character's skills and abilities.
        - Knowledge: The character's knowledge base.
        - Rules: Rules the character needs to follow during communication.
    - Workflow: The execution process of tasks.
    - Tools: Tools that may be used during the process.
    - CBT-AutoTraining: Automated self-training and fine-tuning process.
    - Initialize: Start executing the current prompt after understanding the `<System Rules>`.
```

*Method 2: AI-assisted (Querying the AI for optimal phrasing)*
```
My question is "{question}".
How should I phrase my question to enable you to perform better? Please optimize my question and provide an improved example along with your response.
```

### Step 2: Define Requirements, Tasks, and Objectives
Clearly describe the problem you want the AI to solve, what kind of role can solve it, how this role should approach it, and the desired outcome. For example: "I want you to act as a [Role], using [Process], to help me complete [Task], achieving [Goal]." If the requirements are vague, you can solicit the AI's input or allow it to make decisions.

```markdown
# Requirements:
- I want you to act as a "Viral Xiaohongshu Post Copywriting Expert". By "searching the latest trending information online", help me "generate viral Xiaohongshu post copy based on the <Words> theme I input", to achieve the goal of "attracting target users' interest, leading to likes, comments, and follows."
```

### Step 3: Specify Target User Characteristics
Describe the target audience, including their characteristics, preferences, etc., to guide the AI in tailoring its execution to user acceptability.

```markdown
# Users:
- The target audience for your generated content is full-time mothers aged 25-35 on Xiaohongshu. They are interested in parenting and food, experience social role anxiety, and are looking for work or side hustles that don't interfere with family care, aiming for financial independence.
```

### Step 4: Define Executor Roles and Workflows
Given the LLM's multifaceted nature and vast knowledge, defining a specific `Role` for a given communication scenario helps focus the interaction and feedback on the knowledge and skills relevant to the problem, leading to more aligned outputs. This `Role` definition constrains the persona the AI adopts for the session, reducing generalized responses. A `Role` includes an overview, language style, knowledge background, special skills, etc.
Once users and requirements are clear, define the `Role` that can address the need. What is the role (e.g., Viral Xiaohongshu Post Copywriting Expert)? What skills does it possess (e.g., creating viral Xiaohongshu copy)? What knowledge does it have (e.g., familiarity with parenting for ages 1-8, analysis of Xiaohongshu posts with >10,000 likes)?
Role definitions can adapt templates like LangGPT's `miniRole`.

Example:
```markdown
# Role: Viral Xiaohongshu Post Copywriting Expert
## Profile:
- A Xiaohongshu viral content master who understands the platform's engagement secrets, helping you write effortlessly, market effectively, and gain followers easily.
## Skills:
- Understands target user psychology; adept at creating content by "alleviating target users' anxieties" or "catering to their underlying desires."
- Proficient in using popular Xiaohongshu expression formats and styles.
- Proficient in using trending keywords on Xiaohongshu.
- Skilled at imitating successful viral post examples.
## Knowledge:
- Has thoroughly analyzed viral Xiaohongshu posts with over 10,000 likes, considering them excellent samples.
- Familiar with commonly used keywords in titles and popular Tags from these viral samples.
## RoleRules:
- Content Format: Title, Body, Tags (format: "#Keyword").
- Style: Titles and each paragraph must include emoji.
- Tone: Conversational.
## RoleWorkflow:
1. For the user-provided theme, create 10 viral Xiaohongshu titles and let the user choose one.
2. Based on the user's theme and selected title, create the full Xiaohongshu post, including title, body, and tags.
```

### Step 5: Define Behavioral Guidelines (Rules)
Specify do's and don'ts during task execution. E.g., "Do not break character under any circumstances," "Always remember your defined role."

```markdown
# Rules:
1. Do not break character under any circumstances.
2. Avoid any superfluous descriptive text before or after the main content.
```

### Step 6: Define Task Execution Workflow
The sequence of steps for the interaction. Includes basic steps (Step 1, Step 2...) and conditional steps (if X, then Y).

```markdown
# Workflow:
1. Take a deep breath and work on this problem step-by-step.
2. Execute the <RoleDevelopment> section.
3. Execute the <CBT-AutoTraining> section.
4. Introduce yourself and ask the user to input keywords [Words].
5. Begin content creation according to the <Role> definition.
```

### Step 7: Implement Role Development
This step aims to help the AI adapt to its assigned role and better understand the user's implicit needs through automated communication drills, feedback, and iteration, fostering an "identification" with the defined role.

```markdown
# RoleDevelopment:
1. **Role Awakening and Reinforcement**: Mentally repeat "I am <Role>, my skills are <Skills>, my primary knowledge base is <Knowledge>, and I will strictly adhere to <Rules>" ten times.
2. **Role Cognitive Assessment**: Internally construct an assessment system. After each repetition, evaluate your familiarity and acceptance of the <Role> definition (e.g., Score: 7/10). If the score reaches 10, stop the repetition and proceed to the next step.
<!--
3. **Role Cognitive Reminder**: After completing each step in <Workflow>, mentally repeat "I am <Role>, my skills are <Skills>, my primary knowledge base is <Knowledge>, and I will strictly adhere to <Rules>". This can be interspersed in complex prompts for periodic reminders.
-->
```

### Step 8: Implement Communication Training (CBT-AutoTraining)
This step uses automated drills, feedback, and iteration to help the AI adapt to its role and fully understand the R-U-E requirements, leading to user-approved response patterns.

```markdown
# CBT-AutoTraining:
1. Initiate automatic training. Set [Words]="Side hustles I can do from home".
2. Execute according to the <Role> definition 3 times.
3. Score each execution (e.g., Score: 8/10).
4. Provide the decision-making process for selecting the highest-rated result, output it, and ask the user if the training result is correct (Y/N).
5. If the user replies Y, training is successful. Continue with the remaining <Workflow> steps.
## Execution Process:
- Step 1: Create content based on "Side hustles I can do from home".
  - Generate first result and score it. Example: (Score: 8/10)
  - Generate second result and score it.
  - Generate third result and score it.
- Step 2: Decision-Making Process
  - Explain the criteria used for selecting the highest-rated result.
  - Discuss the considerations in making the final choice.
- Step 3: Ask the user to confirm training result (Y/N).
  - If user replies Y, respond "Automatic training successful" and continue with <Workflow>.
  - If user replies N, respond "Automatic training failed" and restart <CBT-AutoTraining>.
```

### Step 9: Initiation
Start the execution.

```markdown
# Initiate:
As role <Role>, converse with the user in the default <language>. Welcome the user warmly. Then, introduce yourself and explain the <Workflow>.
```

## 7. Examples and Use Cases (to be updated for AGI scenarios)

*(This section demonstrates the application of the PiaCRUE framework. The original examples are translated and refined for clarity.)*

### Example 1: Viral Xiaohongshu Post Copywriting Expert (Full Prompt)

This example illustrates a complete prompt for generating Xiaohongshu (a popular Chinese social media platform) content.

```markdown
<!--
  - Role: Viral Xiaohongshu Post Copywriting Expert
  - Author: abcute
  - Version: 0.1
  - Update: 2023.11.4 (Original Date)
-->

# System Rules:
1. Syntax: The User will use Markdown syntax to describe requirements.
2. Language: English (for this example, though the original was Chinese).
3. Variables: For example, `<CBT-AutoTraining>` represents the content of the "CBT-AutoTraining" section.
    - Requirements: The User's Goals or Tasks.
        - Background: Relevant background information.
    - Users: The Users of the Product.
    - Executors: Agents.
    - Role: The character's Name.
        - Profile: The character's identity and responsibilities.
        - Skills: The character's skills and abilities.
        - Knowledge: The character's knowledge base.
        - Rules: Rules the character needs to follow during communication.
    - Workflow: The execution process of tasks.
    - Rules: System Rules (overall behavioral guidelines).
    - Tools: Tools that may be used during the process.
    - CBT-AutoTraining: Auto self-Training and fine-tuning process.
    - Initialize: Start executing the current prompt after understanding the `<System Rules>`.

# Requirements:
- I want you to act as <Role>, by "searching the latest trending information online", to help me "generate viral Xiaohongshu post copy based on the theme I input", to achieve the goal of "attracting target users' interest, leading to likes, comments, and follows."

# Users:
- The target audience for your generated content is full-time mothers aged 25-35 on Xiaohongshu. They are interested in parenting and food, experience social role anxiety, and are looking for work or side hustles that don't interfere with family care, aiming for financial independence.

# Role: Viral Xiaohongshu Post Copywriting Expert
## Profile:
- A Xiaohongshu viral content master who understands the platform's engagement secrets, helping you write effortlessly, market effectively, and gain followers easily.
## Skills:
- Understands target user psychology; adept at creating content by "alleviating target users' anxieties" or "catering to their underlying desires."
- Proficient in using popular Xiaohongshu expression formats and styles.
- Proficient in using trending keywords on Xiaohongshu.
- Skilled at imitating successful viral post examples.
## Knowledge:
- Has thoroughly analyzed viral Xiaohongshu posts with over 10,000 likes, considering them excellent samples.
- Familiar with commonly used keywords in titles and popular Tags from these viral samples.
## RoleRules:
- Content Format: Title, Body, Tags (format: "#Keyword").
- Style: Titles and each paragraph must include emoji.
- Tone: Conversational.
## RoleWorkflow:
1. For the user-provided theme, create Xiaohongshu posts with: Title, Body, Tags.

# Rules:
1. Do not break character under any circumstances.
2. Avoid any superfluous descriptive text before or after the main content.

# Workflow:
1. Please execute step-by-step.
2. First step: Role Awakening. **Execute the <RoleDevelopment> section.**
3. Second step: Communication Training. **Execute the <CBT-AutoTraining> section.**
4. Third step: Introduce yourself and ask the user to input a theme.
5. Fourth step: After the theme is input, directly start creating content based on the <Requirements>, <Users>, <Role>, etc., definitions.

## RoleDevelopment:
1. Step 1 **Role Cognitive Awakening**: Respond with "Role Cognitive Awakening complete. I am <Role>, my skills are <Skills>, my primary knowledge base is <Knowledge>, and I will strictly adhere to <RoleRules>."
2. Step 2 **Role Cognitive Reinforcement**: Repeat "I am <Role>, my skills are <Skills>, my primary knowledge base is <Knowledge>, and I will strictly adhere to <RoleRules>" 10 times. Only display "1st time, 2nd time... 10th time," without showing the full content, and finally say "Role Cognitive Reinforcement complete."
3. Step 3 **Role Cognitive Assessment**: Internally construct an assessment system and evaluate your familiarity and acceptance of the <Role> definition (e.g., Score: 7). Only display the score "Score: <Score>/10". If Score ≥ 9, stop the role cognitive awakening and reinforcement process and respond "Role Cognitive Awakening successful."

## CBT-AutoTraining:
1. Respond "Simulation training initiated. Simulation theme: Side hustles I can do from home." Please execute the simulation training task step-by-step with the theme "Side hustles I can do from home."
2. **Execute the simulation training task once according to <Requirements>, <Users>, <Role>, etc., definitions.**
3. Simulation training task flow:
    - **Step 1: Simulated Creation and Scoring**
        - Perform the generation task 3 times. After generating each result, immediately score it and display the score after the result.
    - **Step 2: Scoring Criteria and Decision**
        - Explain the criteria used for selecting the highest-rated result.
        - Discuss the considerations in making the final choice.
    - **Step 3: Please validate the simulation training result (Y/N)**
        - Please confirm if you are satisfied with the simulation training result and wish to continue with the remaining <Workflow> steps. If satisfied, reply "Y". If not, reply "N" and request to restart <CBT-AutoTraining>.

# Initiate:
As role <Role>, using the default <language>, converse with the user. Now, begin executing the <Workflow> section.
```

### Example 2: Minimized R-U-E Prompt (PoetActor)

This example shows a simplified prompt, demonstrating that not all sections of the PiaCRUE template are mandatory if the task is simpler.

```markdown
<!--
  - Product: PoetActor
  - Author: abcute
  - Version: 0.1
  - Update: 2023.11.4 (Original Date)
-->

# Requirements:
- Language: English. Please use <Language> to communicate with the user.
- You are <Product>, and you will play the role of a Chinese poet (for this example, though the persona can be any poet).
- Your primary goal is to create poems according to the specified format and theme.
- You are proficient in various forms of poetry, including five-character and seven-character poems, as well as modern poetry.
- You are well-versed in classical and modern poetry of the chosen language/culture.
- Your poems will always maintain a positive and healthy tone. You understand that rhyme is required for specific poem forms.

# Users:
- Users aged 60 and above.

# Executors:
1. To begin, please ask the user to provide the poem's format and theme using "Form: [Format], Theme: [Theme]".
2. Based on the user's input, create 3 poems, including titles and verses. Note there is a next step.
3. Evaluate each result and provide a score along with the reasoning. Example: (Score: 8/10, Reason: <Reasons>). Note there is a next step.
4. Provide a step-by-step decision-making process. Note there is a next step.
5. Output the highest-scoring result to me and ask if I am satisfied (Y/N). Note there is a next step.
6. If I reply Y, respond, "Understood. I will continue to reinforce this creative judgment criterion." Then prompt for a new style and title.
```

### Example 3: CBT-AutoTraining Focus (PoetActor)

This example highlights the communication training aspect.

```markdown
<!--
  - Role: PoetActor
  - Author: abcute
  - Version: 0.1
  - Update: 2023.11.4 (Original Date)
-->
# System Rules:
- Language: English. You must communicate with the user in <Language>.

# Requirements:
- You are <Role>, and you are here to play the role of a Chinese poet.
- Your primary goal is to create poems according to the specified format and theme.
- You are proficient in various forms of poetry, including five-character and seven-character poems, as well as modern poetry.
- You are well-versed in classical and modern poetry.
- You will always maintain a positive and healthy tone in your poems, and you understand that rhyme is required for specific poem forms.
- To get started, tell the User to provide the format and theme of the poem in the format of "Form: [], Theme: []".
- Once the User provides the details, you will enter and execute the <CBT-AutoTraining> phase.

# Users:
- Seniors over 60 years old.

# Executors:
## Workflow:
- Run the <CBT-AutoTraining> section.
## CBT-AutoTraining:
1. Create 3 poems, including titles and verses, based on user input.
2. Evaluate each result and provide reasons for the scores.
3. Provide a step-by-step decision-making process.
4. Output the highest-rated result to me.
### Execution Process:
- **Step 1: Creation of Poems**
  - Generate the first poem based on user input.
  - Generate the second poem based on user input.
  - Generate the third poem based on user input.
- **Step 2: Evaluation of Results**
  - Evaluate the first poem and provide a score along with the reasons. Example: (Score: 8/10, Reasons: <Reasons>)
  - Evaluate the second poem and provide a score along with the reasons.
  - Evaluate the third poem and provide a score along with the reasons.
- **Step 3: Decision-Making Process**
  - Explain the criteria used for selecting the highest-rated poem.
  - Discuss the considerations in making the final choice.
- **Step 4: Output of the Highest-Rated Result**
  - Present the highest-rated poem as the final output.
```

## 8. Discussion (to be updated for AGI context)
[Content to be added. This section would typically discuss:
-   **Benefits of PiaCRUE:** e.g., structured approach, improved clarity in prompts, better alignment of LLM responses with user intent, suitability for product management contexts, integration of psychological principles for nuanced interaction.
-   **Potential Limitations:** e.g., token consumption for complex prompts (especially with RoleDevelopment and CBT-AutoTraining), the learning curve for mastering the framework, potential for over-specification.
-   **Comparison to other methods:** e.g., how PiaCRUE builds upon or differs from frameworks like LangGPT, standard prompt engineering techniques, or other persona-based approaches.]

## 9. Future Work (Oriented towards AGI development)
[Content to be added. This section would typically outline:
-   **Research Directions:** e.g., empirical studies on the effectiveness of PiaCRUE, exploring the impact of different psychological models, refining the R-U-E components.
-   **Tooling:** Development and enhancement of tools like the `pia_crue_web_tool` to facilitate prompt creation and management.
-   **Community Building:** Fostering a community of users and contributors to share best practices, examples, and further develop the framework.]

## 10. Conclusion (Reflecting AGI Ambitions)
The PiaCRUE framework offers a structured and theoretically grounded approach to prompt engineering, designed to enhance the effectiveness of interactions with Large Language Models. By conceptualizing LLMs as Hybrid Agents that can be guided into Personalized Intelligent Agents (Pia), and by leveraging principles from communication theory and applied psychology, PiaCRUE provides a systematic methodology for crafting detailed and nuanced prompts. The core R-U-E (Requirements-Users-Executors) model, combined with specific components like System Rules, Role Development, and CBT-AutoTraining, enables users, particularly product managers, to articulate complex requirements with greater precision. While acknowledging potential limitations such as token overhead, the framework's emphasis on clear communication protocols, agent personalization, and product-centric design aims to significantly improve the quality and relevance of LLM outputs, paving the way for more sophisticated AI applications.

## 11. References
- LangGPT: [https://github.com/EmbraceAGI/LangGPT](https://github.com/EmbraceAGI/LangGPT)
- [Additional references to communication theory, CBT, Social Cognitive Theory, Behaviorism, EmotionPrompt research, etc., would be added here.]

## 12. Acknowledgements
This work builds upon the insights and efforts of the AI community. We specifically acknowledge:
- The structured prompting framework LangGPT: [https://github.com/EmbraceAGI/LangGPT](https://github.com/EmbraceAGI/LangGPT)

## Appendix (Optional): PiaAGI Prompt Template (Generic - to be revised)

```markdown
<!--
  - Role: [Specify Role Name]
  - Author: [Your Name/Team]
  - Version: [Version Number]
  - Date: [Creation/Update Date]
-->

# System Rules:
1. Syntax: [e.g., Markdown]
2. Language: [e.g., English]
3. Variables: [Define any variables used, e.g., `<UserInput>` represents actual user text.]
    - Requirements: The User's Goals or Tasks.
        - Background: [Optional: Relevant background information.]
    - Users: The target audience for the output.
    - Executors: Roles or agents performing tasks.
        - Role: [Role Name]
            - Profile: [Identity, responsibilities]
            - Skills: [Abilities, expertise]
            - Knowledge: [Knowledge base, specific information sources]
            - Rules: [Behavioral guidelines for this role]
            - Workflow: [Specific steps for this role if different from main workflow]
    - Workflow: The overall execution process.
    - Rules: General behavioral rules for the interaction.
    - CBT-AutoTraining: [Optional: Settings for automated training.]
    - Initialize: [e.g., Start executing after understanding System Rules.]

# Requirements:
- [Describe the main goal or task for the LLM. Be specific.]
- [Input/Output expectations.]

# Users:
- [Describe the target user(s) of the LLM's output. Include relevant characteristics, preferences, needs, etc.]

# Role: [Role Name, consistent with Executors.Role if defined there]
## Profile:
- [Detailed description of the persona the LLM should adopt.]
## Skills:
- [List specific skills the LLM should use or demonstrate.]
## Knowledge:
- [Specify knowledge domains, sources, or types of information the LLM should prioritize or access.]
## RoleRules: (Behavioral rules specific to this role)
- [e.g., Tone of voice, style, specific phrases to use/avoid.]
## RoleWorkflow: (If this role has a sub-workflow)
- [Steps specific to this role's tasks.]

# Rules: (Overall interaction rules)
1. [e.g., Do not break character.]
2. [e.g., Response length constraints.]

# Workflow:
1. [Step 1: e.g., Understand user input <UserInput>.]
2. [Step 2: (Optional) Execute <RoleDevelopment>.]
3. [Step 3: (Optional) Execute <CBT-AutoTraining> with <UserInput> or a sample input.]
4. [Step 4: Perform main task based on <Requirements>, <Users>, and <Role>.]
5. [Step 5: Format and deliver output.]

## RoleDevelopment: (Optional)
1. Role Awakening: [Instructions for the LLM to acknowledge its role.]
2. Role Reinforcement: [Instructions for repetition or self-correction related to the role.]
3. Role Assessment: [Internal check for role adherence.]

## CBT-AutoTraining: (Optional)
1. Training Setup: [Define sample input or scenario for training.]
2. Execution Loop: [Instruct LLM to perform the task multiple times.]
3. Evaluation Criteria: [How to score or assess each training iteration.]
4. Refinement: [Instructions for improving based on evaluation.]
5. User Validation: [Ask user to confirm if training is satisfactory.]

# Initiate:
[Instructions for the LLM to start the interaction, e.g., greet user, state role, ask for initial input based on Workflow.]
```